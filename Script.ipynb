{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "975455a7-a289-495e-a0b5-cf6a7c3d33c4",
   "metadata": {},
   "source": [
    "Below is the script which takes the corrected FRAP data and analyses it by outputting the calculated radii using the most probable model. The script is using two types of fit:\n",
    "1. Gradient Boosting Machine (GBM) preprocessing, followed by an exponential fitting. The preprocessing is needed as the data tends to be very noisy, so we train an ML algorithm using the actual data plus additional synthetic data (50% of the original), followed by an exponential fit.\n",
    "2. Direct fit to exponential models without preprocessing.\n",
    "Both methods take a very similar amount of time to be executed, hence there are no additional overheads arising from using the ML model.\n",
    "\n",
    "Additionally, they have virtually identical residuals and goodness of fit parameters, suggesting that they are equal in their precision, however depending on the sample, they deal differently with the outliers.\n",
    "In all cases, the data is fit to 1, 2 and 3 exponential models and then using statistical analysis and ranks, the best model is selected and the correct sizes are outputted on the screen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5d436-504d-4a24-84b9-6f36f933a259",
   "metadata": {},
   "source": [
    "Let's start by importing the required libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be35b289-93a8-43eb-b6c7-4867deda8893",
   "metadata": {},
   "source": [
    "We need to edit path entries in cells: [4], [9], [12], [19], [31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c8418-ae8f-4382-803c-9e44db26443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867487c0-929d-414a-9929-e55890f5a4fe",
   "metadata": {},
   "source": [
    "Next, we define how many experimental runs there are in the data set and what are the cut-off parameters for outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d1aad-d6f2-4e50-8ac4-99eaaf8387a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of runs\n",
    "num_runs = 10\n",
    "\n",
    "# Define the number of standard deviations for outlier detection\n",
    "num_std = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0b215-4885-439d-afe8-f88703ac6de7",
   "metadata": {},
   "source": [
    "We also need to define the experimental parameters and some sample characteristics to get precise values for the R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c01911-4d0f-4946-81a2-c8fbc9a52c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperature = 293.15 # kelvin\n",
    "viscosity = 0.0146 #define viscosity in Pa*s\n",
    "param = 0.88 #correction factor for the gaussian beam profile\n",
    "w = 10e-6 #radius of the bleaching spot\n",
    "kb = 1.38e-23 # definition for Boltzmann constant\n",
    "timestep = 0.2 #imaging timestep in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaf803f-9e6d-4d18-b807-307c75289461",
   "metadata": {},
   "source": [
    "Lastly, we need to load up an Excel spreadsheet that contains our raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26fbaf-fa2f-45c2-8733-0ea69bda8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the provided Excel file\n",
    "file_path = r\"E:\\LudoxMixturesDataSheets\\HSAS_70-30\\5uM_R6G\\FRAPCurves.xlsx\"\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a74fa-cc4a-4001-8169-c431f94fdc6b",
   "metadata": {},
   "source": [
    "and define our exponential models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337713e-fbcc-478f-9ed4-2abd569cb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exponential models\n",
    "def exp_model_single(t, b0, b1, tau1):\n",
    "    return b0 + b1 * (1 - np.exp(-t / tau1))\n",
    "\n",
    "def exp_model_double(t, b0, b1, tau1, b2, tau2):\n",
    "    return b0 + b1 * (1 - np.exp(-t / tau1)) + b2 * (1 - np.exp(-t / tau2))\n",
    "\n",
    "def exp_model_triple(t, b0, b1, tau1, b2, tau2, b3, tau3):\n",
    "    return b0 + b1 * (1 - np.exp(-t / tau1)) + b2 * (1 - np.exp(-t / tau2)) + b3 * (1 - np.exp(-t / tau3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d101b-859b-4847-a3c5-c6c550783d51",
   "metadata": {},
   "source": [
    "First, we start from a GBM preprocessed fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344fe88-d42c-49bb-b30c-bf2b445263b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frame = df['Frame'].values\n",
    "runs = [df[f'Run_{i}'].values for i in range(1, num_runs + 1)]\n",
    "\n",
    "# Initialize arrays to store results for all runs\n",
    "tau_single_values = np.zeros(num_runs)\n",
    "A_single_values = np.zeros(num_runs)\n",
    "tau_double_values = np.zeros((num_runs, 2))\n",
    "A_double_values = np.zeros((num_runs, 2))\n",
    "tau_triple_values = np.zeros((num_runs, 3))\n",
    "A_triple_values = np.zeros((num_runs, 3))\n",
    "\n",
    "chisq_single_values = np.zeros(num_runs)\n",
    "chisq_double_values = np.zeros(num_runs)\n",
    "chisq_triple_values = np.zeros(num_runs)\n",
    "\n",
    "aicc_single_values = np.zeros(num_runs)\n",
    "aicc_double_values = np.zeros(num_runs)\n",
    "aicc_triple_values = np.zeros(num_runs)\n",
    "\n",
    "for run_idx in range(num_runs):\n",
    "    intensity = runs[run_idx][50:]\n",
    "    time = Frame[50:]\n",
    "    # Remove NaN values\n",
    "    valid_mask = ~np.isnan(intensity)\n",
    "    intensity = intensity[valid_mask]\n",
    "    time = time[valid_mask]\n",
    "\n",
    "    # Generate synthetic data using bootstrapping\n",
    "    synthetic_time, synthetic_intensity = resample(time, intensity, n_samples=int(len(time) * 0.5), random_state=69)\n",
    "\n",
    "    # Combine real and synthetic data\n",
    "    combined_time = np.concatenate([time, synthetic_time])\n",
    "    combined_intensity = np.concatenate([intensity, synthetic_intensity])\n",
    "\n",
    "    # Gradient Boosting Machine regression on combined data\n",
    "    gbm = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3)\n",
    "    gbm.fit(combined_time.reshape(-1, 1), combined_intensity)\n",
    "    predicted_intensity = gbm.predict(time.reshape(-1, 1))\n",
    "\n",
    "    # Single exponential model fitting\n",
    "    beta_single, _ = curve_fit(exp_model_single, time, predicted_intensity, p0=[min(predicted_intensity), max(predicted_intensity) - min(predicted_intensity), 100])\n",
    "\n",
    "    # Double exponential model fitting\n",
    "    beta_double, _ = curve_fit(exp_model_double, time, predicted_intensity, p0=[min(predicted_intensity), (max(predicted_intensity) - min(predicted_intensity)) / 2, 30, (max(predicted_intensity) - min(predicted_intensity)) / 2, 150])\n",
    "\n",
    "    # Triple exponential model fitting with constraints\n",
    "    bounds = (0, [300, 300, 300, 300, 300, 300, 300])\n",
    "    beta_triple, _ = curve_fit(exp_model_triple, time, predicted_intensity, p0=[min(predicted_intensity), (max(predicted_intensity) - min(predicted_intensity)) / 3, 20, (max(predicted_intensity) - min(predicted_intensity)) / 3, 50, (max(predicted_intensity) - min(predicted_intensity)) / 3, 200], bounds=bounds)\n",
    "\n",
    "    # Extract tau and A values\n",
    "    tau_single = beta_single[2]\n",
    "    A_single = beta_single[1]\n",
    "\n",
    "    tau_double = np.sort([beta_double[2], beta_double[4]])\n",
    "    A_double = [beta_double[1], beta_double[3]]\n",
    "    A_double = np.array(A_double)[np.argsort([beta_double[2], beta_double[4]])]\n",
    "\n",
    "    tau_triple = np.sort([beta_triple[2], beta_triple[4], beta_triple[6]])\n",
    "    A_triple = [beta_triple[1], beta_triple[3], beta_triple[5]]\n",
    "    A_triple = np.array(A_triple)[np.argsort([beta_triple[2], beta_triple[4], beta_triple[6]])]\n",
    "\n",
    "    # Calculate chi-squared errors\n",
    "    fitted_intensity_single = exp_model_single(time, *beta_single)\n",
    "    fitted_intensity_double = exp_model_double(time, *beta_double)\n",
    "    fitted_intensity_triple = exp_model_triple(time, *beta_triple)\n",
    "\n",
    "    chisq_single = np.sum((intensity - fitted_intensity_single) ** 2)\n",
    "    chisq_double = np.sum((intensity - fitted_intensity_double) ** 2)\n",
    "    chisq_triple = np.sum((intensity - fitted_intensity_triple) ** 2)\n",
    "\n",
    "    # Calculate AIC and AICc\n",
    "    n = len(time)\n",
    "    k_single = 3\n",
    "    k_double = 5\n",
    "    k_triple = 7\n",
    "\n",
    "    aic_single = n * np.log(chisq_single / n) + 2 * k_single\n",
    "    aic_double = n * np.log(chisq_double / n) + 2 * k_double\n",
    "    aic_triple = n * np.log(chisq_triple / n) + 2 * k_triple\n",
    "\n",
    "    aicc_single = aic_single + (2 * k_single ** 2 + 2 * k_single) / (n - k_single - 1)\n",
    "    aicc_double = aic_double + (2 * k_double ** 2 + 2 * k_double) / (n - k_double - 1)\n",
    "    aicc_triple = aic_triple + (2 * k_triple ** 2 + 2 * k_triple) / (n - k_triple - 1)\n",
    "\n",
    "    # Store tau, A, and chi-squared values\n",
    "    tau_single_values[run_idx] = tau_single\n",
    "    A_single_values[run_idx] = A_single\n",
    "    tau_double_values[run_idx, :] = tau_double\n",
    "    A_double_values[run_idx, :] = A_double\n",
    "    tau_triple_values[run_idx, :] = tau_triple\n",
    "    A_triple_values[run_idx, :] = A_triple\n",
    "\n",
    "    chisq_single_values[run_idx] = chisq_single\n",
    "    chisq_double_values[run_idx] = chisq_double\n",
    "    chisq_triple_values[run_idx] = chisq_triple\n",
    "\n",
    "    aicc_single_values[run_idx] = aicc_single\n",
    "    aicc_double_values[run_idx] = aicc_double\n",
    "    aicc_triple_values[run_idx] = aicc_triple\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918a29a-d133-425b-97d3-aab4153fa4f2",
   "metadata": {},
   "source": [
    "Optionally, we ca plot the fit and results for each run. To do it, uncomment all of the lines below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3bf792-da68-4c00-a0c7-c3af4ff1312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Display the results for the current run\n",
    "#    print(f'Results for Run_{run_idx + 1}:')\n",
    "#    print(f'Single Exponential: Tau = {tau_single}, A = {A_single}, Chi-squared = {chisq_single}, AICc = {aicc_single}')\n",
    "#    print(f'Double Exponential: Tau = {tau_double}, A = {A_double}, Chi-squared = {chisq_double}, AICc = {aicc_double}')\n",
    "#    print(f'Triple Exponential: Tau = {tau_triple}, A = {A_triple}, Chi-squared = {chisq_triple}, AICc = {aicc_triple}')\n",
    "\n",
    "    # Plot the data and the fitted curves\n",
    "#    plt.figure()\n",
    "#    plt.plot(Frame, runs[run_idx], 'o', label='Data')\n",
    "#    plt.plot(time, predicted_intensity, 'k-', label='GBM Predicted Intensity', linewidth=2)\n",
    "#    plt.plot(time, fitted_intensity_single, '-', label='Single Exponential Fit', linewidth=2)\n",
    "#    plt.plot(time, fitted_intensity_double, '--', label='Double Exponential Fit', linewidth=2)\n",
    "#    plt.plot(time, fitted_intensity_triple, '-.', label='Triple Exponential Fit', linewidth=2)\n",
    "#    plt.xlabel('Time')\n",
    "#    plt.ylabel('Intensity')\n",
    "#    plt.legend()\n",
    "#    plt.title(f'Run_{run_idx + 1}: Data and Fitted Curves')\n",
    "#    plt.grid(True)\n",
    "#    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61216b77-3d04-4ddc-8aee-495c57859079",
   "metadata": {},
   "source": [
    "Next, we will be looking for the outliers. The idea is rather simple:\n",
    "1. We find the most probable value of the parameters and calculate the mean\n",
    "2. If any of the parameters is 2 standard deviations away from the mean, we flag it as an outlier\n",
    "3. The same is done for every model, i.e., single, double and triple exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb955a20-3246-4867-85f5-28443d37df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine parameters into a single array for easier processing\n",
    "parameters = {\n",
    "    'Tau_Single': tau_single_values,\n",
    "    'A_Single': A_single_values,\n",
    "    'Tau_Double1': tau_double_values[:, 0],\n",
    "    'Tau_Double2': tau_double_values[:, 1],\n",
    "    'A_Double1': A_double_values[:, 0],\n",
    "    'A_Double2': A_double_values[:, 1],\n",
    "    'Tau_Triple1': tau_triple_values[:, 0],\n",
    "    'Tau_Triple2': tau_triple_values[:, 1],\n",
    "    'Tau_Triple3': tau_triple_values[:, 2],\n",
    "    'A_Triple1': A_triple_values[:, 0],\n",
    "    'A_Triple2': A_triple_values[:, 1],\n",
    "    'A_Triple3': A_triple_values[:, 2]\n",
    "}\n",
    "\n",
    "# Calculate mean and standard deviation for each parameter\n",
    "mean_values = {key: np.mean(val) for key, val in parameters.items()}\n",
    "std_values = {key: np.std(val) for key, val in parameters.items()}\n",
    "\n",
    "# Detect outliers based on 2 standard deviations and additional conditions\n",
    "outliers = {key: np.abs(val - mean_values[key]) > num_std * std_values[key] for key, val in parameters.items()}\n",
    "\n",
    "# Additional conditions:\n",
    "# 1. If any two tau values are equal\n",
    "equal_tau_double = np.isclose(tau_double_values[:, 0], tau_double_values[:, 1])\n",
    "equal_tau_triple = np.isclose(tau_triple_values[:, 0], tau_triple_values[:, 1]) | \\\n",
    "                   np.isclose(tau_triple_values[:, 0], tau_triple_values[:, 2]) | \\\n",
    "                   np.isclose(tau_triple_values[:, 1], tau_triple_values[:, 2])\n",
    "\n",
    "# 2. If any amplitude (A) is above 1\n",
    "A_above_1_single = A_single_values > 1\n",
    "A_above_1_double = (A_double_values[:, 0] > 1) | (A_double_values[:, 1] > 1)\n",
    "A_above_1_triple = (A_triple_values[:, 0] > 1) | (A_triple_values[:, 1] > 1) | (A_triple_values[:, 2] > 1)\n",
    "\n",
    "# 3. If any amplitude (A) is negative\n",
    "A_negative_single = A_single_values < 0\n",
    "A_negative_double = (A_double_values[:, 0] < 0) | (A_double_values[:, 1] < 0)\n",
    "A_negative_triple = (A_triple_values[:, 0] < 0) | (A_triple_values[:, 1] < 0) | (A_triple_values[:, 2] < 0)\n",
    "\n",
    "# Combine all outlier conditions\n",
    "outlier_runs = np.any(np.column_stack([\n",
    "    outliers['Tau_Single'], \n",
    "    outliers['A_Single'], \n",
    "    outliers['Tau_Double1'], \n",
    "    outliers['Tau_Double2'], \n",
    "    outliers['A_Double1'], \n",
    "    outliers['A_Double2'], \n",
    "    outliers['Tau_Triple1'], \n",
    "    outliers['Tau_Triple2'], \n",
    "    outliers['Tau_Triple3'], \n",
    "    outliers['A_Triple1'], \n",
    "    outliers['A_Triple2'], \n",
    "    outliers['A_Triple3'],\n",
    "    equal_tau_double,  # Additional condition 1: Equal Tau for double-exponential\n",
    "    equal_tau_triple,  # Additional condition 1: Equal Tau for triple-exponential\n",
    "    A_above_1_single,  # Additional condition 2: A > 1 for single-exponential\n",
    "    A_above_1_double,  # Additional condition 2: A > 1 for double-exponential\n",
    "    A_above_1_triple,  # Additional condition 2: A > 1 for triple-exponential\n",
    "    A_negative_single,  # Additional condition 3: Negative A for single-exponential\n",
    "    A_negative_double,  # Additional condition 3: Negative A for double-exponential\n",
    "    A_negative_triple   # Additional condition 3: Negative A for triple-exponential\n",
    "]), axis=1)\n",
    "\n",
    "# Display outlier runs\n",
    "print(f'Outlier Runs: {np.where(outlier_runs)[0] + 1}')\n",
    "\n",
    "# Update summary data with the new outlier conditions and AICc values\n",
    "summary_data = {\n",
    "    'Run': np.arange(1, num_runs + 1),\n",
    "    'Tau_Single': tau_single_values,\n",
    "    'A_Single': A_single_values,\n",
    "    'AICc_Single': aicc_single_values,\n",
    "    'Outlier_Single': outliers['Tau_Single'] | outliers['A_Single'] | A_above_1_single | A_negative_single,\n",
    "    'Tau_Double1': tau_double_values[:, 0],\n",
    "    'Tau_Double2': tau_double_values[:, 1],\n",
    "    'A_Double1': A_double_values[:, 0],\n",
    "    'A_Double2': A_double_values[:, 1],\n",
    "    'AICc_Double': aicc_double_values,\n",
    "    'Outlier_Double': outliers['Tau_Double1'] | outliers['Tau_Double2'] | outliers['A_Double1'] | outliers['A_Double2'] |\n",
    "                      equal_tau_double | A_above_1_double | A_negative_double,\n",
    "    'Tau_Triple1': tau_triple_values[:, 0],\n",
    "    'Tau_Triple2': tau_triple_values[:, 1],\n",
    "    'Tau_Triple3': tau_triple_values[:, 2],\n",
    "    'A_Triple1': A_triple_values[:, 0],\n",
    "    'A_Triple2': A_triple_values[:, 1],\n",
    "    'A_Triple3': A_triple_values[:, 2],\n",
    "    'AICc_Triple': aicc_triple_values,\n",
    "    'Outlier_Triple': outliers['Tau_Triple1'] | outliers['Tau_Triple2'] | outliers['Tau_Triple3'] |\n",
    "                      outliers['A_Triple1'] | outliers['A_Triple2'] | outliers['A_Triple3'] |\n",
    "                      equal_tau_triple | A_above_1_triple | A_negative_triple\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa9c031-cf22-4e0a-99c9-280c0b074a69",
   "metadata": {},
   "source": [
    "Lastly, we create a summary Excel file which will be used for next steps and can also be used for any manual analysis if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31faed-33cc-4573-a8c4-a29a94e148da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform your analysis and create the summary DataFrame (assuming `summary_data` is already generated)\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Extract the directory from the file_path\n",
    "directory = os.path.dirname(file_path)\n",
    "\n",
    "# Create the new file name for the summary file\n",
    "summary_file_name = \"FRAP_5uM_Summary_with_Additional_Outliers.xlsx\"\n",
    "\n",
    "# Construct the full path for saving the summary file\n",
    "summary_file_path = os.path.join(directory, summary_file_name)\n",
    "\n",
    "# Save the summary table to the new Excel file\n",
    "summary_df.to_excel(summary_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce519c9-dc4e-4a60-91af-31054896530b",
   "metadata": {},
   "source": [
    "Now, for comparison, we will do the same, but using a direct fitting to exponential models, without GBM preprocessing part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564a21b-1c9e-43a1-bd19-e29bfc26ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store results for all runs\n",
    "tau_single_values_direct = np.zeros(num_runs)\n",
    "A_single_values_direct = np.zeros(num_runs)\n",
    "tau_double_values_direct = np.zeros((num_runs, 2))\n",
    "A_double_values_direct = np.zeros((num_runs, 2))\n",
    "tau_triple_values_direct = np.zeros((num_runs, 3))\n",
    "A_triple_values_direct = np.zeros((num_runs, 3))\n",
    "\n",
    "chisq_single_values_direct = np.zeros(num_runs)\n",
    "chisq_double_values_direct = np.zeros(num_runs)\n",
    "chisq_triple_values_direct = np.zeros(num_runs)\n",
    "\n",
    "aicc_single_values_direct = np.zeros(num_runs)\n",
    "aicc_double_values_direct = np.zeros(num_runs)\n",
    "aicc_triple_values_direct = np.zeros(num_runs)\n",
    "\n",
    "for run_idx in range(num_runs):\n",
    "    intensity = runs[run_idx][50:]\n",
    "    time = Frame[50:]\n",
    "    # Remove NaN values\n",
    "    valid_mask = ~np.isnan(intensity)\n",
    "    intensity = intensity[valid_mask]\n",
    "    time = time[valid_mask]\n",
    "\n",
    "    # Direct exponential model fitting without GBM\n",
    "\n",
    "    # Single exponential model fitting\n",
    "    beta_single_direct, _ = curve_fit(exp_model_single, time, intensity, p0=[min(intensity), max(intensity) - min(intensity), 100])\n",
    "\n",
    "    # Double exponential model fitting\n",
    "    beta_double_direct, _ = curve_fit(exp_model_double, time, intensity, p0=[min(intensity), (max(intensity) - min(intensity)) / 2, 30, (max(intensity) - min(intensity)) / 2, 150])\n",
    "\n",
    "    # Triple exponential model fitting with constraints\n",
    "    bounds = (0, [300, 300, 300, 300, 300, 300, 300])\n",
    "    beta_triple_direct, _ = curve_fit(exp_model_triple, time, intensity, p0=[min(intensity), (max(intensity) - min(intensity)) / 3, 20, (max(intensity) - min(intensity)) / 3, 50, (max(intensity) - min(intensity)) / 3, 200], bounds=bounds)\n",
    "\n",
    "    # Extract tau and A values\n",
    "    tau_single_direct = beta_single_direct[2]\n",
    "    A_single_direct = beta_single_direct[1]\n",
    "\n",
    "    tau_double_direct = np.sort([beta_double_direct[2], beta_double_direct[4]])\n",
    "    A_double_direct = [beta_double_direct[1], beta_double_direct[3]]\n",
    "    A_double_direct = np.array(A_double_direct)[np.argsort([beta_double_direct[2], beta_double_direct[4]])]\n",
    "\n",
    "    tau_triple_direct = np.sort([beta_triple_direct[2], beta_triple_direct[4], beta_triple_direct[6]])\n",
    "    A_triple_direct = [beta_triple_direct[1], beta_triple_direct[3], beta_triple_direct[5]]\n",
    "    A_triple_direct = np.array(A_triple_direct)[np.argsort([beta_triple_direct[2], beta_triple_direct[4], beta_triple_direct[6]])]\n",
    "\n",
    "    # Calculate chi-squared errors for direct fitting\n",
    "    fitted_intensity_single_direct = exp_model_single(time, *beta_single_direct)\n",
    "    fitted_intensity_double_direct = exp_model_double(time, *beta_double_direct)\n",
    "    fitted_intensity_triple_direct = exp_model_triple(time, *beta_triple_direct)\n",
    "\n",
    "    chisq_single_direct = np.sum((intensity - fitted_intensity_single_direct) ** 2)\n",
    "    chisq_double_direct = np.sum((intensity - fitted_intensity_double_direct) ** 2)\n",
    "    chisq_triple_direct = np.sum((intensity - fitted_intensity_triple_direct) ** 2)\n",
    "\n",
    "    # Calculate AIC and AICc for direct fitting\n",
    "    n = len(time)\n",
    "    k_single = 3\n",
    "    k_double = 5\n",
    "    k_triple = 7\n",
    "\n",
    "    aic_single_direct = n * np.log(chisq_single_direct / n) + 2 * k_single\n",
    "    aic_double_direct = n * np.log(chisq_double_direct / n) + 2 * k_double\n",
    "    aic_triple_direct = n * np.log(chisq_triple_direct / n) + 2 * k_triple\n",
    "\n",
    "    aicc_single_direct = aic_single_direct + (2 * k_single ** 2 + 2 * k_single) / (n - k_single - 1)\n",
    "    aicc_double_direct = aic_double_direct + (2 * k_double ** 2 + 2 * k_double) / (n - k_double - 1)\n",
    "    aicc_triple_direct = aic_triple_direct + (2 * k_triple ** 2 + 2 * k_triple) / (n - k_triple - 1)\n",
    "\n",
    "    # Store tau, A, and chi-squared values for direct fitting\n",
    "    tau_single_values_direct[run_idx] = tau_single_direct\n",
    "    A_single_values_direct[run_idx] = A_single_direct\n",
    "    tau_double_values_direct[run_idx, :] = tau_double_direct\n",
    "    A_double_values_direct[run_idx, :] = A_double_direct\n",
    "    tau_triple_values_direct[run_idx, :] = tau_triple_direct\n",
    "    A_triple_values_direct[run_idx, :] = A_triple_direct\n",
    "\n",
    "    chisq_single_values_direct[run_idx] = chisq_single_direct\n",
    "    chisq_double_values_direct[run_idx] = chisq_double_direct\n",
    "    chisq_triple_values_direct[run_idx] = chisq_triple_direct\n",
    "\n",
    "    aicc_single_values_direct[run_idx] = aicc_single_direct\n",
    "    aicc_double_values_direct[run_idx] = aicc_double_direct\n",
    "    aicc_triple_values_direct[run_idx] = aicc_triple_direct\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3321c60-0e6d-413e-9d6c-cc1c7438d274",
   "metadata": {},
   "source": [
    "And we also look for outliers using the exact same method as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ade678-6664-47b2-9e5f-a97cb1e73846",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dictionary with parameters for outlier detection\n",
    "parameters = {\n",
    "    'Tau_Single_Direct': tau_single_values_direct,\n",
    "    'A_Single_Direct': A_single_values_direct,\n",
    "    'Tau_Double1_Direct': tau_double_values_direct[:, 0],\n",
    "    'Tau_Double2_Direct': tau_double_values_direct[:, 1],\n",
    "    'A_Double1_Direct': A_double_values_direct[:, 0],\n",
    "    'A_Double2_Direct': A_double_values_direct[:, 1],\n",
    "    'Tau_Triple1_Direct': tau_triple_values_direct[:, 0],\n",
    "    'Tau_Triple2_Direct': tau_triple_values_direct[:, 1],\n",
    "    'Tau_Triple3_Direct': tau_triple_values_direct[:, 2],\n",
    "    'A_Triple1_Direct': A_triple_values_direct[:, 0],\n",
    "    'A_Triple2_Direct': A_triple_values_direct[:, 1],\n",
    "    'A_Triple3_Direct': A_triple_values_direct[:, 2]\n",
    "}\n",
    "\n",
    "# Calculate mean and standard deviation for each parameter\n",
    "mean_values = {key: np.mean(val) for key, val in parameters.items()}\n",
    "std_values = {key: np.std(val) for key, val in parameters.items()}\n",
    "\n",
    "# Detect outliers based on 2 standard deviations and additional conditions\n",
    "outliers_direct = {key: np.abs(val - mean_values[key]) > num_std * std_values[key] for key, val in parameters.items()}\n",
    "\n",
    "# Additional conditions\n",
    "equal_tau_double_direct = np.isclose(parameters['Tau_Double1_Direct'], parameters['Tau_Double2_Direct'])\n",
    "equal_tau_triple_direct = np.isclose(parameters['Tau_Triple1_Direct'], parameters['Tau_Triple2_Direct']) | \\\n",
    "                          np.isclose(parameters['Tau_Triple1_Direct'], parameters['Tau_Triple3_Direct']) | \\\n",
    "                          np.isclose(parameters['Tau_Triple2_Direct'], parameters['Tau_Triple3_Direct'])\n",
    "\n",
    "A_above_1_single_direct = parameters['A_Single_Direct'] > 1\n",
    "A_above_1_double_direct = (parameters['A_Double1_Direct'] > 1) | (parameters['A_Double2_Direct'] > 1)\n",
    "A_above_1_triple_direct = (parameters['A_Triple1_Direct'] > 1) | (parameters['A_Triple2_Direct'] > 1) | (parameters['A_Triple3_Direct'] > 1)\n",
    "\n",
    "A_negative_single_direct = parameters['A_Single_Direct'] < 0\n",
    "A_negative_double_direct = (parameters['A_Double1_Direct'] < 0) | (parameters['A_Double2_Direct'] < 0)\n",
    "A_negative_triple_direct = (parameters['A_Triple1_Direct'] < 0) | (parameters['A_Triple2_Direct'] < 0) | (parameters['A_Triple3_Direct'] < 0)\n",
    "\n",
    "# Combine all outlier conditions\n",
    "outlier_runs_direct = np.any(np.column_stack([\n",
    "    outliers_direct['Tau_Single_Direct'], \n",
    "    outliers_direct['A_Single_Direct'], \n",
    "    outliers_direct['Tau_Double1_Direct'], \n",
    "    outliers_direct['Tau_Double2_Direct'], \n",
    "    outliers_direct['A_Double1_Direct'], \n",
    "    outliers_direct['A_Double2_Direct'], \n",
    "    outliers_direct['Tau_Triple1_Direct'], \n",
    "    outliers_direct['Tau_Triple2_Direct'], \n",
    "    outliers_direct['Tau_Triple3_Direct'], \n",
    "    outliers_direct['A_Triple1_Direct'], \n",
    "    outliers_direct['A_Triple2_Direct'], \n",
    "    outliers_direct['A_Triple3_Direct'],\n",
    "    equal_tau_double_direct,  # Additional condition 1: Equal Tau for double-exponential\n",
    "    equal_tau_triple_direct,  # Additional condition 1: Equal Tau for triple-exponential\n",
    "    A_above_1_single_direct,  # Additional condition 2: A > 1 for single-exponential\n",
    "    A_above_1_double_direct,  # Additional condition 2: A > 1 for double-exponential\n",
    "    A_above_1_triple_direct,  # Additional condition 2: A > 1 for triple-exponential\n",
    "    A_negative_single_direct,  # Additional condition 3: Negative A for single-exponential\n",
    "    A_negative_double_direct,  # Additional condition 3: Negative A for double-exponential\n",
    "    A_negative_triple_direct   # Additional condition 3: Negative A for triple-exponential\n",
    "]), axis=1)\n",
    "\n",
    "# Display outlier runs\n",
    "print(f'Outlier Runs: {np.where(outlier_runs_direct)[0] + 1}')\n",
    "# Update summary data with the new outlier conditions and AICc values\n",
    "summary_data_direct = {\n",
    "    'Run': np.arange(1, num_runs + 1),\n",
    "    'Tau_Single_Direct': tau_single_values_direct,\n",
    "    'A_Single_Direct': A_single_values_direct,\n",
    "    'AICc_Single_Direct': aicc_single_values_direct,  # Add AICc Single\n",
    "    'Outlier_Single_Direct': outliers_direct['Tau_Single_Direct'] | outliers_direct['A_Single_Direct'] | A_above_1_single_direct | A_negative_single_direct,\n",
    "    'Tau_Double1_Direct': tau_double_values_direct[:, 0],\n",
    "    'Tau_Double2_Direct': tau_double_values_direct[:, 1],\n",
    "    'A_Double1_Direct': A_double_values_direct[:, 0],\n",
    "    'A_Double2_Direct': A_double_values_direct[:, 1],\n",
    "    'AICc_Double_Direct': aicc_double_values_direct,  # Add AICc Double\n",
    "    'Outlier_Double_Direct': outliers_direct['Tau_Double1_Direct'] | outliers_direct['Tau_Double2_Direct'] | outliers_direct['A_Double1_Direct'] | outliers_direct['A_Double2_Direct'] |\n",
    "                             equal_tau_double_direct | A_above_1_double_direct | A_negative_double_direct,\n",
    "    'Tau_Triple1_Direct': tau_triple_values_direct[:, 0],\n",
    "    'Tau_Triple2_Direct': tau_triple_values_direct[:, 1],\n",
    "    'Tau_Triple3_Direct': tau_triple_values_direct[:, 2],\n",
    "    'A_Triple1_Direct': A_triple_values_direct[:, 0],\n",
    "    'A_Triple2_Direct': A_triple_values_direct[:, 1],\n",
    "    'A_Triple3_Direct': A_triple_values_direct[:, 2],\n",
    "    'AICc_Triple_Direct': aicc_triple_values_direct,  # Add AICc Triple\n",
    "    'Outlier_Triple_Direct': outliers_direct['Tau_Triple1_Direct'] | outliers_direct['Tau_Triple2_Direct'] | outliers_direct['Tau_Triple3_Direct'] |\n",
    "                             outliers_direct['A_Triple1_Direct'] | outliers_direct['A_Triple2_Direct'] | outliers_direct['A_Triple3_Direct'] |\n",
    "                             equal_tau_triple_direct | A_above_1_triple_direct | A_negative_triple_direct\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a7ede-bec9-4baa-b88f-cc5c13717d21",
   "metadata": {},
   "source": [
    "And generate a summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd1eb3-7bd4-494c-8868-201bc56fb786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming summary_data_direct is already generated\n",
    "summary_df_direct = pd.DataFrame(summary_data_direct)\n",
    "\n",
    "# Extract the directory from the original file path\n",
    "file_path = r\"E:\\LudoxMixturesDataSheets\\HSAS_70-30\\5uM_R6G\\FRAPCurves.xlsx\"\n",
    "directory = os.path.dirname(file_path)\n",
    "\n",
    "# Create the new file name for the summary file\n",
    "summary_file_name = \"FRAP_5uM_Summary_with_Additional_Outliers_Direct.xlsx\"\n",
    "\n",
    "# Construct the full path for saving the summary file\n",
    "summary_file_path = os.path.join(directory, summary_file_name)\n",
    "\n",
    "# Save the summary table to the new Excel file\n",
    "summary_df_direct.to_excel(summary_file_path, index=False)\n",
    "\n",
    "# Display the summary table\n",
    "print(summary_df_direct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b82665-916f-4f31-9896-1e5f00aed5c7",
   "metadata": {},
   "source": [
    "Now that the data is fitted, we ignore the outliers and focus on the good data only for all future steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d976d6-c095-4c1a-a232-4b56093bf77e",
   "metadata": {},
   "source": [
    "First, we convert the recovered Tau values into recovery half times by multiplying them by ln(2) and by our imaging timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d320fa-d671-4194-a277-7f33e46bdc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract non-outlier indices for each model\n",
    "non_outlier_single = ~outliers['Tau_Single'] & ~A_above_1_single & ~A_negative_single\n",
    "non_outlier_double1 = ~outliers['Tau_Double1'] & ~A_above_1_double & ~A_negative_double & ~equal_tau_double\n",
    "non_outlier_double2 = ~outliers['Tau_Double2'] & ~A_above_1_double & ~A_negative_double & ~equal_tau_double\n",
    "non_outlier_triple1 = ~outliers['Tau_Triple1'] & ~A_above_1_triple & ~A_negative_triple & ~equal_tau_triple\n",
    "non_outlier_triple2 = ~outliers['Tau_Triple2'] & ~A_above_1_triple & ~A_negative_triple & ~equal_tau_triple\n",
    "non_outlier_triple3 = ~outliers['Tau_Triple3'] & ~A_above_1_triple & ~A_negative_triple & ~equal_tau_triple\n",
    "\n",
    "# Perform the calculations for each model component\n",
    "tau_single_non_outlier = tau_single_values[non_outlier_single]\n",
    "tau_double1_non_outlier = tau_double_values[non_outlier_double1, 0]\n",
    "tau_double2_non_outlier = tau_double_values[non_outlier_double2, 1]\n",
    "tau_triple1_non_outlier = tau_triple_values[non_outlier_triple1, 0]\n",
    "tau_triple2_non_outlier = tau_triple_values[non_outlier_triple2, 1]\n",
    "tau_triple3_non_outlier = tau_triple_values[non_outlier_triple3, 2]\n",
    "\n",
    "# Calculate T * ln(2) * timestep\n",
    "ln2 = np.log(2)\n",
    "result_single = tau_single_non_outlier * ln2 * timestep\n",
    "result_double1 = tau_double1_non_outlier * ln2 * timestep\n",
    "result_double2 = tau_double2_non_outlier * ln2 * timestep\n",
    "result_triple1 = tau_triple1_non_outlier * ln2 * timestep\n",
    "result_triple2 = tau_triple2_non_outlier * ln2 * timestep\n",
    "result_triple3 = tau_triple3_non_outlier * ln2 * timestep\n",
    "\n",
    "# Print results\n",
    "print(\"Single Exponential T values (non-outliers):\")\n",
    "print(result_single)\n",
    "\n",
    "print(\"Double Exponential T values (non-outliers) - Tau1:\")\n",
    "print(result_double1)\n",
    "\n",
    "print(\"Double Exponential T values (non-outliers) - Tau2:\")\n",
    "print(result_double2)\n",
    "\n",
    "print(\"Triple Exponential T values (non-outliers) - Tau1:\")\n",
    "print(result_triple1)\n",
    "\n",
    "print(\"Triple Exponential T values (non-outliers) - Tau2:\")\n",
    "print(result_triple2)\n",
    "\n",
    "print(\"Triple Exponential T values (non-outliers) - Tau3:\")\n",
    "print(result_triple3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a1d04-818c-45f1-8754-4eb85bd297f2",
   "metadata": {},
   "source": [
    "And do the same for the direct fit as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8412104-b043-4de2-a918-f89a0fc71488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract non-outlier indices for each model (Directed fits)\n",
    "non_outlier_single_direct = ~outliers_direct['Tau_Single_Direct'] & ~A_above_1_single_direct & ~A_negative_single_direct\n",
    "non_outlier_double1_direct = ~outliers_direct['Tau_Double1_Direct'] & ~A_above_1_double_direct & ~A_negative_double_direct & ~equal_tau_double_direct\n",
    "non_outlier_double2_direct = ~outliers_direct['Tau_Double2_Direct'] & ~A_above_1_double_direct & ~A_negative_double_direct & ~equal_tau_double_direct\n",
    "non_outlier_triple1_direct = ~outliers_direct['Tau_Triple1_Direct'] & ~A_above_1_triple_direct & ~A_negative_triple_direct & ~equal_tau_triple_direct\n",
    "non_outlier_triple2_direct = ~outliers_direct['Tau_Triple2_Direct'] & ~A_above_1_triple_direct & ~A_negative_triple_direct & ~equal_tau_triple_direct\n",
    "non_outlier_triple3_direct = ~outliers_direct['Tau_Triple3_Direct'] & ~A_above_1_triple_direct & ~A_negative_triple_direct & ~equal_tau_triple_direct\n",
    "\n",
    "# Perform the calculations for each model component (Directed fits)\n",
    "tau_single_direct_non_outlier = tau_single_values_direct[non_outlier_single_direct]\n",
    "tau_double1_direct_non_outlier = tau_double_values_direct[non_outlier_double1_direct, 0]\n",
    "tau_double2_direct_non_outlier = tau_double_values_direct[non_outlier_double2_direct, 1]\n",
    "tau_triple1_direct_non_outlier = tau_triple_values_direct[non_outlier_triple1_direct, 0]\n",
    "tau_triple2_direct_non_outlier = tau_triple_values_direct[non_outlier_triple2_direct, 1]\n",
    "tau_triple3_direct_non_outlier = tau_triple_values_direct[non_outlier_triple3_direct, 2]\n",
    "\n",
    "# Calculate T * ln(2) * timestep for Directed fits\n",
    "ln2 = np.log(2)\n",
    "result_single_direct = tau_single_direct_non_outlier * ln2 * timestep\n",
    "result_double1_direct = tau_double1_direct_non_outlier * ln2 * timestep\n",
    "result_double2_direct = tau_double2_direct_non_outlier * ln2 * timestep\n",
    "result_triple1_direct = tau_triple1_direct_non_outlier * ln2 * timestep\n",
    "result_triple2_direct = tau_triple2_direct_non_outlier * ln2 * timestep\n",
    "result_triple3_direct = tau_triple3_direct_non_outlier * ln2 * timestep\n",
    "\n",
    "# Print results for Directed fits\n",
    "print(\"Single Exponential T values (non-outliers) - Directed:\")\n",
    "print(result_single_direct)\n",
    "\n",
    "print(\"Double Exponential T values (non-outliers) - Directed - Tau1:\")\n",
    "print(result_double1_direct)\n",
    "\n",
    "print(\"Double Exponential T values (non-outliers) - Directed - Tau2:\")\n",
    "print(result_double2_direct)\n",
    "\n",
    "print(\"Triple Exponential T values (non-outliers) - Directed - Tau1:\")\n",
    "print(result_triple1_direct)\n",
    "\n",
    "print(\"Triple Exponential T values (non-outliers) - Directed - Tau2:\")\n",
    "print(result_triple2_direct)\n",
    "\n",
    "print(\"Triple Exponential T values (non-outliers) - Directed - Tau3:\")\n",
    "print(result_triple3_direct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fa38cf-564f-4d50-afde-716ddf69dcdc",
   "metadata": {},
   "source": [
    "Next, we calculate the diffusion coefficients for both.\n",
    "First we do the DBM pre-processed model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15256de-3db8-4b0d-8f04-c793a0a79a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate diffusion coefficients using the non-outlier values\n",
    "D_single = param * ((w ** 2) / (4 * result_single))\n",
    "D_double1 = param * (w ** 2) / (4 * result_double1)\n",
    "D_double2 = param * (w ** 2) / (4 * result_double2)\n",
    "D_triple1 = param * (w ** 2) / (4 * result_triple1)\n",
    "D_triple2 = param * (w ** 2) / (4 * result_triple2)\n",
    "D_triple3 = param * (w ** 2) / (4 * result_triple3)\n",
    "\n",
    "# Print diffusion coefficients\n",
    "print(\"Diffusion Coefficients for Single Exponential (non-outliers):\")\n",
    "print(D_single)\n",
    "\n",
    "print(\"Diffusion Coefficients for Double Exponential (non-outliers) - Tau1:\")\n",
    "print(D_double1)\n",
    "\n",
    "print(\"Diffusion Coefficients for Double Exponential (non-outliers) - Tau2:\")\n",
    "print(D_double2)\n",
    "\n",
    "print(\"Diffusion Coefficients for Triple Exponential (non-outliers) - Tau1:\")\n",
    "print(D_triple1)\n",
    "\n",
    "print(\"Diffusion Coefficients for Triple Exponential (non-outliers) - Tau2:\")\n",
    "print(D_triple2)\n",
    "\n",
    "print(\"Diffusion Coefficients for Triple Exponential (non-outliers) - Tau3:\")\n",
    "print(D_triple3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91821620-81ef-43ce-99a5-7ff405609da1",
   "metadata": {},
   "source": [
    "And for the direct fittings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02292ef9-6b01-4511-a648-b8306f7a4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate diffusion coefficients using the non-outlier values from Direct Fitting\n",
    "D_single_direct = param * ((w ** 2) / (4 * result_single_direct))\n",
    "D_double1_direct = param * (w ** 2) / (4 * result_double1_direct)\n",
    "D_double2_direct = param * (w ** 2) / (4 * result_double2_direct)\n",
    "D_triple1_direct = param * (w ** 2) / (4 * result_triple1_direct)\n",
    "D_triple2_direct = param * (w ** 2) / (4 * result_triple2_direct)\n",
    "D_triple3_direct = param * (w ** 2) / (4 * result_triple3_direct)\n",
    "\n",
    "# Print diffusion coefficients for Direct Fitting\n",
    "print(\"Diffusion Coefficients for Single Exponential (non-outliers) - Direct:\")\n",
    "print(D_single_direct)\n",
    "\n",
    "print(\"Diffusion Coefficients for Double Exponential (non-outliers) - Tau1 - Direct:\")\n",
    "print(D_double1_direct)\n",
    "\n",
    "print(\"Diffusion Coefficients for Double Exponential (non-outliers) - Tau2 - Direct:\")\n",
    "print(D_double2_direct)\n",
    "\n",
    "print(\"Diffusion Coefficients for Triple Exponential (non-outliers) - Tau1 - Direct:\")\n",
    "print(D_triple1_direct)\n",
    "\n",
    "print(\"Diffusion Coefficients for Triple Exponential (non-outliers) - Tau2 - Direct:\")\n",
    "print(D_triple2_direct)\n",
    "\n",
    "print(\"Diffusion Coefficients for Triple Exponential (non-outliers) - Tau3 - Direct:\")\n",
    "print(D_triple3_direct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6bb2d0-0575-48f8-b81b-7d6f797f7136",
   "metadata": {},
   "source": [
    "Lastly, we calculate the hydrodynamic radius in every non-outlier case, starting with the GBM pre-processed model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2514e-7cb8-446f-a2e4-24b13ea1fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate particle sizes using the diffusion coefficients\n",
    "R_single = (kb * Temperature) / (6 * np.pi * viscosity * D_single) / 1e-9\n",
    "R_double1 = (kb * Temperature) / (6 * np.pi * viscosity * D_double1) / 1e-9\n",
    "R_double2 = (kb * Temperature) / (6 * np.pi * viscosity * D_double2) / 1e-9\n",
    "R_triple1 = (kb * Temperature) / (6 * np.pi * viscosity * D_triple1) / 1e-9\n",
    "R_triple2 = (kb * Temperature) / (6 * np.pi * viscosity * D_triple2) / 1e-9\n",
    "R_triple3 = (kb * Temperature) / (6 * np.pi * viscosity * D_triple3) / 1e-9\n",
    "\n",
    "# Print particle sizes\n",
    "print(\"Particle Sizes for Single Exponential (non-outliers):\")\n",
    "print(R_single)\n",
    "\n",
    "print(\"Particle Sizes for Double Exponential (non-outliers) - Tau1:\")\n",
    "print(R_double1)\n",
    "\n",
    "print(\"Particle Sizes for Double Exponential (non-outliers) - Tau2:\")\n",
    "print(R_double2)\n",
    "\n",
    "print(\"Particle Sizes for Triple Exponential (non-outliers) - Tau1:\")\n",
    "print(R_triple1)\n",
    "\n",
    "print(\"Particle Sizes for Triple Exponential (non-outliers) - Tau2:\")\n",
    "print(R_triple2)\n",
    "\n",
    "print(\"Particle Sizes for Triple Exponential (non-outliers) - Tau3:\")\n",
    "print(R_triple3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d06a07-6b5b-40ea-b655-af21197058c5",
   "metadata": {},
   "source": [
    "And now do the same for the direct fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0826350c-2d77-44de-a045-1efd7f165a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick reminder:\n",
    "# Constants (Ensure these are defined beforehand)\n",
    "# kb = Boltzmann constant\n",
    "# Temperature = system temperature (in Kelvin)\n",
    "# viscosity = viscosity of the medium (in Pa.s or equivalent)\n",
    "\n",
    "# Calculate particle sizes using the diffusion coefficients from Direct Fitting\n",
    "R_single_direct = (kb * Temperature) / (6 * np.pi * viscosity * D_single_direct) / 1e-9\n",
    "R_double1_direct = (kb * Temperature) / (6 * np.pi * viscosity * D_double1_direct) / 1e-9\n",
    "R_double2_direct = (kb * Temperature) / (6 * np.pi * viscosity * D_double2_direct) / 1e-9\n",
    "R_triple1_direct = (kb * Temperature) / (6 * np.pi * viscosity * D_triple1_direct) / 1e-9\n",
    "R_triple2_direct = (kb * Temperature) / (6 * np.pi * viscosity * D_triple2_direct) / 1e-9\n",
    "R_triple3_direct = (kb * Temperature) / (6 * np.pi * viscosity * D_triple3_direct) / 1e-9\n",
    "\n",
    "# Print particle sizes for Direct Fitting\n",
    "print(\"Particle Sizes for Single Exponential (non-outliers) - Direct:\")\n",
    "print(R_single_direct)\n",
    "\n",
    "print(\"Particle Sizes for Double Exponential (non-outliers) - Tau1 - Direct:\")\n",
    "print(R_double1_direct)\n",
    "\n",
    "print(\"Particle Sizes for Double Exponential (non-outliers) - Tau2 - Direct:\")\n",
    "print(R_double2_direct)\n",
    "\n",
    "print(\"Particle Sizes for Triple Exponential (non-outliers) - Tau1 - Direct:\")\n",
    "print(R_triple1_direct)\n",
    "\n",
    "print(\"Particle Sizes for Triple Exponential (non-outliers) - Tau2 - Direct:\")\n",
    "print(R_triple2_direct)\n",
    "\n",
    "print(\"Particle Sizes for Triple Exponential (non-outliers) - Tau3 - Direct:\")\n",
    "print(R_triple3_direct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a42f0b-a4c5-466b-b3c6-3098c0acb9eb",
   "metadata": {},
   "source": [
    "Now, we have fitted all of the data set using a total of six models:\n",
    "\n",
    "1. GBM Single-Exponential\n",
    "2. GBM Double-Exponential\n",
    "3. GBM Triple-Exponential\n",
    "4. Direct Single-Exponential\n",
    "5. Direct Double-Exponential\n",
    "6. Direct Triple-Exponential\n",
    "\n",
    "Now we need to identify which model is the best and plot the results. To do this, we will be using AICc, Standard Deviation and the Likelihood ratio test. \n",
    "\n",
    "Let's calculate all of that step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b1517-10da-4ae8-9096-935462cda857",
   "metadata": {},
   "source": [
    "First lest rank the models based on the average AIC results after excluding the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81630410-dea1-48c4-849c-4a157198f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the summary data for GBM and direct fitting\n",
    "summary_df = pd.read_excel(r\"E:\\LudoxMixturesDataSheets\\HSAS_70-30\\5uM_R6G\\FRAP_5uM_Summary_with_Additional_Outliers.xlsx\")\n",
    "summary_df_direct = pd.read_excel(r\"E:\\LudoxMixturesDataSheets\\HSAS_70-30\\5uM_R6G\\FRAP_5uM_Summary_with_Additional_Outliers_Direct.xlsx\")\n",
    "\n",
    "# Filter out the non-outlier data\n",
    "non_outlier_df = summary_df[~summary_df['Outlier_Single']]\n",
    "non_outlier_df_direct = summary_df_direct[~summary_df_direct['Outlier_Single_Direct']]\n",
    "\n",
    "# Calculate the mean AIC values for non-outlier data\n",
    "mean_aic_single = non_outlier_df['AICc_Single'].mean()\n",
    "mean_aic_double = non_outlier_df['AICc_Double'].mean()\n",
    "mean_aic_triple = non_outlier_df['AICc_Triple'].mean()\n",
    "\n",
    "mean_aic_single_direct = non_outlier_df_direct['AICc_Single_Direct'].mean()\n",
    "mean_aic_double_direct = non_outlier_df_direct['AICc_Double_Direct'].mean()\n",
    "mean_aic_triple_direct = non_outlier_df_direct['AICc_Triple_Direct'].mean()\n",
    "\n",
    "# Compare models based on mean AIC values\n",
    "aic_values = {\n",
    "    'GBM_Single': mean_aic_single,\n",
    "    'GBM_Double': mean_aic_double,\n",
    "    'GBM_Triple': mean_aic_triple,\n",
    "    'Direct_Single': mean_aic_single_direct,\n",
    "    'Direct_Double': mean_aic_double_direct,\n",
    "    'Direct_Triple': mean_aic_triple_direct\n",
    "}\n",
    "\n",
    "# Rank the models from best to worst based on mean AIC values\n",
    "ranked_models = sorted(aic_values.items(), key=lambda x: x[1])\n",
    "\n",
    "# Display the ranking\n",
    "print(\"Model Ranking based on mean AIC values:\")\n",
    "for rank, (model, aic) in enumerate(ranked_models, start=1):\n",
    "    print(f\"{rank}. {model}: AICc = {aic:.4f}\")\n",
    "\n",
    "# State which model is the best\n",
    "best_model, best_aic = ranked_models[0]\n",
    "print(f'\\nThe best model based on the lowest mean AIC value is: {best_model} with AIC value: {best_aic:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb534a75-1bbe-4dec-ac77-87752825d04c",
   "metadata": {},
   "source": [
    "However, the AIC values on its own are not enough as they just show how well the model fits the experimental data without accounting for the consistency of the model, i.e., how robust it is when estimating parameters and how consistent they are. As a result, we need extra validation. \n",
    "\n",
    "1. Let's calculate the difference in standard deviations and combine it with the AIC. We will add additional constraints by saying that 10% difference in standard deviation and 40 points in AIC do not yield any significance, hence can be ignored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94d16f-e263-4e21-95f5-68c53ca93471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define standard deviations for Direct model\n",
    "std_tau_single_direct = np.std(non_outlier_df_direct['Tau_Single_Direct'])\n",
    "std_tau_double1_direct = np.std(non_outlier_df_direct['Tau_Double1_Direct'])\n",
    "std_tau_double2_direct = np.std(non_outlier_df_direct['Tau_Double2_Direct'])\n",
    "std_tau_triple1_direct = np.std(non_outlier_df_direct['Tau_Triple1_Direct'])\n",
    "std_tau_triple2_direct = np.std(non_outlier_df_direct['Tau_Triple2_Direct'])\n",
    "std_tau_triple3_direct = np.std(non_outlier_df_direct['Tau_Triple3_Direct'])\n",
    "\n",
    "# Define standard deviations for GBM model\n",
    "std_tau_single = np.std(non_outlier_df['Tau_Single'])\n",
    "std_tau_double1 = np.std(non_outlier_df['Tau_Double1'])\n",
    "std_tau_double2 = np.std(non_outlier_df['Tau_Double2'])\n",
    "std_tau_triple1 = np.std(non_outlier_df['Tau_Triple1'])\n",
    "std_tau_triple2 = np.std(non_outlier_df['Tau_Triple2'])\n",
    "std_tau_triple3 = np.std(non_outlier_df['Tau_Triple3'])\n",
    "\n",
    "# Compare standard deviations and suggest which method is better\n",
    "def compare_std(std_direct, std_gbm, param_name):\n",
    "    if abs(std_direct - std_gbm) / std_gbm < 0.10:  # Less than 10% difference\n",
    "        return (f'Standard deviation for {param_name} is not significantly different between methods.', False)\n",
    "    elif std_direct < std_gbm:\n",
    "        return (f'The Direct method is better for {param_name} with a standard deviation of {std_direct:.4f} compared to GBM method with {std_gbm:.4f}.', True)\n",
    "    else:\n",
    "        return (f'The GBM method is better for {param_name} with a standard deviation of {std_gbm:.4f} compared to Direct method with {std_direct:.4f}.', False)\n",
    "\n",
    "# Perform the comparison\n",
    "std_comparison = {\n",
    "    'Tau_Single': compare_std(std_tau_single_direct, std_tau_single, 'Tau_Single'),\n",
    "    'Tau_Double1': compare_std(std_tau_double1_direct, std_tau_double1, 'Tau_Double1'),\n",
    "    'Tau_Double2': compare_std(std_tau_double2_direct, std_tau_double2, 'Tau_Double2'),\n",
    "    'Tau_Triple1': compare_std(std_tau_triple1_direct, std_tau_triple1, 'Tau_Triple1'),\n",
    "    'Tau_Triple2': compare_std(std_tau_triple2_direct, std_tau_triple2, 'Tau_Triple2'),\n",
    "    'Tau_Triple3': compare_std(std_tau_triple3_direct, std_tau_triple3, 'Tau_Triple3')\n",
    "}\n",
    "\n",
    "# Print standard deviation comparison results\n",
    "for param_name, (comparison, is_direct_better) in std_comparison.items():\n",
    "    print(comparison)\n",
    "\n",
    "# Combine AICc and StdDev for a final decision\n",
    "def combined_ranking(mean_aic, std_dev, mean_aic_direct, std_dev_direct, aic_weight=0.5, std_weight=0.5, aic_threshold=40, std_threshold=0.10):\n",
    "    ranking = {\n",
    "        'GBM_Single': (mean_aic['Single'], std_dev['Single']),\n",
    "        'GBM_Double': (mean_aic['Double'], std_dev['Double']),\n",
    "        'GBM_Triple': (mean_aic['Triple'], std_dev['Triple']),\n",
    "        'Direct_Single': (mean_aic_direct['Single'], std_dev_direct['Single']),\n",
    "        'Direct_Double': (mean_aic_direct['Double'], std_dev_direct['Double']),\n",
    "        'Direct_Triple': (mean_aic_direct['Triple'], std_dev_direct['Triple'])\n",
    "    }\n",
    "    \n",
    "    # Calculate a combined score with weights for AICc and standard deviation\n",
    "    scores = {}\n",
    "    for model, (aic, std) in ranking.items():\n",
    "        if 'Direct' in model:\n",
    "            aic_value = mean_aic_direct[model.split('_')[1]]\n",
    "            std_value = std_dev_direct[model.split('_')[1]]\n",
    "        else:\n",
    "            aic_value = mean_aic[model.split('_')[1]]\n",
    "            std_value = std_dev[model.split('_')[1]]\n",
    "        \n",
    "        # Apply thresholds\n",
    "        for other_model, (other_aic, other_std) in ranking.items():\n",
    "            if model != other_model:\n",
    "                if abs(aic_value - other_aic) < aic_threshold:\n",
    "                    aic_value = min(aic_value, other_aic)  # Use the lower AICc if within threshold\n",
    "                if abs(std_value - other_std) / other_std < std_threshold:\n",
    "                    std_value = min(std_value, other_std)  # Use the lower std dev if within threshold\n",
    "        \n",
    "        score = aic_weight * aic_value + std_weight * std_value\n",
    "        scores[model] = score\n",
    "    \n",
    "    # Sort by combined score (lower score is better)\n",
    "    sorted_ranking = sorted(scores.items(), key=lambda x: x[1])\n",
    "    return sorted_ranking\n",
    "\n",
    "# Define AICc and Standard Deviations (replace these with actual values)\n",
    "mean_aic = {\n",
    "    'Single': mean_aic_single,\n",
    "    'Double': mean_aic_double,\n",
    "    'Triple': mean_aic_triple\n",
    "}\n",
    "\n",
    "mean_aic_direct = {\n",
    "    'Single': mean_aic_single_direct,\n",
    "    'Double': mean_aic_double_direct,\n",
    "    'Triple': mean_aic_triple_direct\n",
    "}\n",
    "\n",
    "std_dev = {\n",
    "    'Single': std_tau_single,\n",
    "    'Double': (std_tau_double1 + std_tau_double2) / 2,  # Average for Double\n",
    "    'Triple': (std_tau_triple1 + std_tau_triple2 + std_tau_triple3) / 3  # Average for Triple\n",
    "}\n",
    "\n",
    "std_dev_direct = {\n",
    "    'Single': std_tau_single_direct,\n",
    "    'Double': (std_tau_double1_direct + std_tau_double2_direct) / 2,\n",
    "    'Triple': (std_tau_triple1_direct + std_tau_triple2_direct + std_tau_triple3_direct) / 3\n",
    "}\n",
    "\n",
    "# Get combined ranking\n",
    "final_ranking = combined_ranking(mean_aic, std_dev, mean_aic_direct, std_dev_direct, aic_weight=0.6, std_weight=0.4, aic_threshold=40, std_threshold=0.10)\n",
    "\n",
    "# Display the final ranking\n",
    "print(\"\\nFinal Model Ranking based on Combined AICc and Standard Deviation:\")\n",
    "for rank, (model, score) in enumerate(final_ranking, start=1):\n",
    "    aic_value = mean_aic_direct[model.split('_')[1]] if 'Direct' in model else mean_aic[model.split('_')[1]]\n",
    "    std_value = std_dev_direct[model.split('_')[1]] if 'Direct' in model else std_dev[model.split('_')[1]]\n",
    "    print(f\"{rank}. {model}: AICc = {aic_value:.4f}, StdDev = {std_value:.4f}, Combined Score = {score:.4f}\")\n",
    "\n",
    "# Best model\n",
    "best_model, best_score = final_ranking[0]\n",
    "best_aic = mean_aic_direct[best_model.split('_')[1]] if 'Direct' in best_model else mean_aic[best_model.split('_')[1]]\n",
    "best_std = std_dev_direct[best_model.split('_')[1]] if 'Direct' in best_model else std_dev[model.split('_')[1]]\n",
    "print(f'\\nThe best model based on the combined criteria is: {best_model} with AICc = {best_aic:.4f}, Standard Deviation = {best_std:.4f}, and Combined Score = {best_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c300066-492e-441a-86e9-2253eea625e9",
   "metadata": {},
   "source": [
    "Finally, lets calculate the likelihood and add that to our ranking as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ddf85-eb02-4ec7-9d98-2d1bd8c1178d",
   "metadata": {},
   "source": [
    "First we do it for GBM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce5694d-621b-481d-80b6-8f61ee476d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume `outlier_runs` is a boolean array indicating which runs are outliers\n",
    "\n",
    "def calculate_log_likelihood(chisq, n, k):\n",
    "    \"\"\"\n",
    "    Calculate the log-likelihood from the chi-squared value.\n",
    "    \n",
    "    Parameters:\n",
    "    - chisq: Array of chi-squared values\n",
    "    - n: Number of observations\n",
    "    - k: Number of parameters\n",
    "    \n",
    "    Returns:\n",
    "    - log_likelihood: Array of log-likelihood values\n",
    "    \"\"\"\n",
    "    # To handle potential log of zero or negative chi-squared values\n",
    "    chisq[chisq <= 0] = np.nan\n",
    "    log_likelihood = -0.5 * (n * np.log(2 * np.pi) + n * np.log(chisq / n) + n)\n",
    "    return log_likelihood\n",
    "\n",
    "# Number of observations\n",
    "n = len(time)\n",
    "\n",
    "# Number of parameters\n",
    "k_single = 3\n",
    "k_double = 5\n",
    "k_triple = 7\n",
    "\n",
    "# Filter out the outlier runs\n",
    "non_outlier_indices = ~outlier_runs\n",
    "\n",
    "# Calculate log-likelihoods only for non-outlier runs\n",
    "log_likelihood_single = calculate_log_likelihood(chisq_single_values[non_outlier_indices], n, k_single)\n",
    "log_likelihood_double = calculate_log_likelihood(chisq_double_values[non_outlier_indices], n, k_double)\n",
    "log_likelihood_triple = calculate_log_likelihood(chisq_triple_values[non_outlier_indices], n, k_triple)\n",
    "\n",
    "# Print log-likelihood values\n",
    "print(f'Log-Likelihood Single Exponential (non-outliers): {log_likelihood_single}')\n",
    "print(f'Log-Likelihood Double Exponential (non-outliers): {log_likelihood_double}')\n",
    "print(f'Log-Likelihood Triple Exponential (non-outliers): {log_likelihood_triple}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6835c037-b0bb-4382-b8b2-bd987ac36b21",
   "metadata": {},
   "source": [
    "And then for the direct model as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b7c7de-6302-443c-bfb3-b38812b0ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_likelihood(chisq, n, k):\n",
    "    \"\"\"\n",
    "    Calculate the log-likelihood from the chi-squared value.\n",
    "    \n",
    "    Parameters:\n",
    "    - chisq: Array of chi-squared values\n",
    "    - n: Number of observations\n",
    "    - k: Number of parameters\n",
    "    \n",
    "    Returns:\n",
    "    - log_likelihood: Array of log-likelihood values\n",
    "    \"\"\"\n",
    "    # Handle potential log of zero or negative chi-squared values\n",
    "    chisq[chisq <= 0] = np.nan\n",
    "    log_likelihood = -0.5 * (n * np.log(2 * np.pi) + n * np.log(chisq / n) + n)\n",
    "    return log_likelihood\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have already defined 'outlier_runs_direct'\n",
    "# Filter out the outlier runs\n",
    "non_outlier_indices = ~outlier_runs_direct\n",
    "\n",
    "# Number of observations (update if necessary)\n",
    "n = len(time)  # Make sure 'time' is the same for all runs or adjust accordingly\n",
    "\n",
    "# Number of parameters\n",
    "k_single = 3\n",
    "k_double = 5\n",
    "k_triple = 7\n",
    "\n",
    "# Calculate log-likelihoods only for non-outlier runs\n",
    "log_likelihood_single_direct = calculate_log_likelihood(chisq_single_values_direct[non_outlier_indices], n, k_single)\n",
    "log_likelihood_double_direct = calculate_log_likelihood(chisq_double_values_direct[non_outlier_indices], n, k_double)\n",
    "log_likelihood_triple_direct = calculate_log_likelihood(chisq_triple_values_direct[non_outlier_indices], n, k_triple)\n",
    "\n",
    "# Print log-likelihood values\n",
    "print(f'Log-Likelihood Single Exponential (non-outliers): {log_likelihood_single_direct}')\n",
    "print(f'Log-Likelihood Double Exponential (non-outliers): {log_likelihood_double_direct}')\n",
    "print(f'Log-Likelihood Triple Exponential (non-outliers): {log_likelihood_triple_direct}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c29430-9099-43fe-844b-cdcb4896d3af",
   "metadata": {},
   "source": [
    "Finally, lets combine all of the parameters into a single score that would tell us which model is the best, with the weights for each parameter as follows:\n",
    "1. AIC = 0.3\n",
    "2. Standard Deviation = 0.4\n",
    "3. Likelihood = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee49358-0d9c-49de-94cd-f10ace6b1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_ranking(mean_aic, std_dev, log_likelihood, mean_aic_direct, std_dev_direct, log_likelihood_direct,\n",
    "                     aic_weight=0.3, std_weight=0.4, log_like_weight=0.3, aic_threshold=40, std_threshold=0.10):\n",
    "    ranking = {\n",
    "        'GBM_Single': (mean_aic['Single'], std_dev['Single'], log_likelihood['Single']),\n",
    "        'GBM_Double': (mean_aic['Double'], std_dev['Double'], log_likelihood['Double']),\n",
    "        'GBM_Triple': (mean_aic['Triple'], std_dev['Triple'], log_likelihood['Triple']),\n",
    "        'Direct_Single': (mean_aic_direct['Single'], std_dev_direct['Single'], log_likelihood_direct['Single']),\n",
    "        'Direct_Double': (mean_aic_direct['Double'], std_dev_direct['Double'], log_likelihood_direct['Double']),\n",
    "        'Direct_Triple': (mean_aic_direct['Triple'], std_dev_direct['Triple'], log_likelihood_direct['Triple'])\n",
    "    }\n",
    "    \n",
    "    # Calculate a combined score with weights for AICc, standard deviation, and log-likelihood\n",
    "    scores = {}\n",
    "    for model, (aic, std, log_like) in ranking.items():\n",
    "        # Apply thresholds for AICc and standard deviation\n",
    "        for other_model, (other_aic, other_std, other_log_like) in ranking.items():\n",
    "            if model != other_model:\n",
    "                if abs(aic - other_aic) < aic_threshold:\n",
    "                    aic = min(aic, other_aic)  # Use the lower AICc if within threshold\n",
    "                if abs(std - other_std) / other_std < std_threshold:\n",
    "                    std = min(std, other_std)  # Use the lower std dev if within threshold\n",
    "        \n",
    "        score = (aic_weight * aic) + (std_weight * std) - (log_like_weight * log_like)  # Note: log-likelihood is subtracted\n",
    "        scores[model] = score\n",
    "    \n",
    "    # Sort by combined score (lower score is better)\n",
    "    sorted_ranking = sorted(scores.items(), key=lambda x: x[1])\n",
    "    return sorted_ranking\n",
    "\n",
    "# Define AICc, Standard Deviations, and Log-Likelihoods\n",
    "mean_aic = {\n",
    "    'Single': mean_aic_single,\n",
    "    'Double': mean_aic_double,\n",
    "    'Triple': mean_aic_triple\n",
    "}\n",
    "\n",
    "mean_aic_direct = {\n",
    "    'Single': mean_aic_single_direct,\n",
    "    'Double': mean_aic_double_direct,\n",
    "    'Triple': mean_aic_triple_direct\n",
    "}\n",
    "\n",
    "std_dev = {\n",
    "    'Single': std_tau_single,\n",
    "    'Double': (std_tau_double1 + std_tau_double2) / 2,  # Average for Double\n",
    "    'Triple': (std_tau_triple1 + std_tau_triple2 + std_tau_triple3) / 3  # Average for Triple\n",
    "}\n",
    "\n",
    "std_dev_direct = {\n",
    "    'Single': std_tau_single_direct,\n",
    "    'Double': (std_tau_double1_direct + std_tau_double2_direct) / 2,\n",
    "    'Triple': (std_tau_triple1_direct + std_tau_triple2_direct + std_tau_triple3_direct) / 3\n",
    "}\n",
    "\n",
    "log_likelihood = {\n",
    "    'Single': np.mean(log_likelihood_single),\n",
    "    'Double': np.mean(log_likelihood_double),\n",
    "    'Triple': np.mean(log_likelihood_triple)\n",
    "}\n",
    "\n",
    "log_likelihood_direct = {\n",
    "    'Single': np.mean(log_likelihood_single_direct),\n",
    "    'Double': np.mean(log_likelihood_double_direct),\n",
    "    'Triple': np.mean(log_likelihood_triple_direct)\n",
    "}\n",
    "\n",
    "# Get combined ranking\n",
    "final_ranking = combined_ranking(mean_aic, std_dev, log_likelihood, mean_aic_direct, std_dev_direct, log_likelihood_direct, aic_weight=0.4, std_weight=0.3, log_like_weight=0.3, aic_threshold=40, std_threshold=0.10)\n",
    "\n",
    "# Display the final ranking\n",
    "print(\"\\nFinal Model Ranking based on Combined AICc, Standard Deviation, and Log-Likelihood:\")\n",
    "for rank, (model, score) in enumerate(final_ranking, start=1):\n",
    "    aic_value = mean_aic_direct[model.split('_')[1]] if 'Direct' in model else mean_aic[model.split('_')[1]]\n",
    "    std_value = std_dev_direct[model.split('_')[1]] if 'Direct' in model else std_dev[model.split('_')[1]]\n",
    "    log_like_value = log_likelihood_direct[model.split('_')[1]] if 'Direct' in model else log_likelihood[model.split('_')[1]]\n",
    "    print(f\"{rank}. {model}: AICc = {aic_value:.4f}, StdDev = {std_value:.4f}, Log-Likelihood = {log_like_value:.4f}, Combined Score = {score:.4f}\")\n",
    "\n",
    "# Best model\n",
    "best_model, best_score = final_ranking[0]\n",
    "best_aic = mean_aic_direct[best_model.split('_')[1]] if 'Direct' in best_model else mean_aic[best_model.split('_')[1]]\n",
    "best_std = std_dev_direct[best_model.split('_')[1]] if 'Direct' in best_model else std_dev[best_model.split('_')[1]]\n",
    "best_log_like = log_likelihood_direct[best_model.split('_')[1]] if 'Direct' in best_model else log_likelihood[best_model.split('_')[1]]\n",
    "print(f'\\nThe best model based on the combined criteria is: {best_model} with AICc = {best_aic:.4f}, Standard Deviation = {best_std:.4f}, Log-Likelihood = {best_log_like:.4f}, and Combined Score = {best_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6244c9-7988-4597-a234-50274097dc86",
   "metadata": {},
   "source": [
    "Now that we have the model ranking, let's fit all of the results so we can select the appropriate one at the end.\n",
    "First, let's go with the GBM approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68718a3-584f-4281-89a2-08a1d3d8c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the non-outlier runs are identified and outlier_runs is available\n",
    "non_outlier_mask = ~outlier_runs  # Inverse mask for non-outliers\n",
    "non_outlier_runs = [runs[i] for i in range(len(runs)) if non_outlier_mask[i]]\n",
    "non_outlier_runs = np.array(non_outlier_runs)\n",
    "# Get the time values (assuming the time axis is consistent across all runs)\n",
    "time = Frame[50:]\n",
    "\n",
    "# Calculate the average intensity, ignoring NaN values\n",
    "average_intensity = np.nanmean(non_outlier_runs[:, 50:], axis=0)\n",
    "\n",
    "# Extend time and intensity to include the initial prebleached phase\n",
    "extended_time = np.concatenate([Frame[:50], time])\n",
    "extended_average_intensity = np.concatenate([np.nanmean([run[:50] for run in non_outlier_runs], axis=0), average_intensity])\n",
    "\n",
    "# Create a mask based on the valid (non-NaN) average intensity values\n",
    "valid_mask = ~np.isnan(extended_average_intensity)\n",
    "\n",
    "# Apply the mask to both time and average intensity to ensure they match in size\n",
    "extended_time = extended_time[valid_mask]\n",
    "extended_average_intensity = extended_average_intensity[valid_mask]\n",
    "\n",
    "# Define indices to separate bleach phase and post-bleach phase\n",
    "bleach_index = np.where(Frame == 50)[0][0]  # Adjust this if bleach phase index is different\n",
    "\n",
    "# Fit the models to the average intensity from the bleach phase onwards\n",
    "time_bleach = extended_time[bleach_index:]\n",
    "average_intensity_bleach = extended_average_intensity[bleach_index:]\n",
    "\n",
    "# Fit the single exponential model to the average curve\n",
    "beta_single_avg, _ = curve_fit(exp_model_single, time_bleach, average_intensity_bleach,\n",
    "                               p0=[min(average_intensity_bleach), max(average_intensity_bleach) - min(average_intensity_bleach), 100])\n",
    "\n",
    "# Fit the double exponential model to the average curve\n",
    "beta_double_avg, _ = curve_fit(exp_model_double, time_bleach, average_intensity_bleach,\n",
    "                               p0=[min(average_intensity_bleach), (max(average_intensity_bleach) - min(average_intensity_bleach)) / 2, 30,\n",
    "                                   (max(average_intensity_bleach) - min(average_intensity_bleach)) / 2, 150])\n",
    "\n",
    "# Fit the triple exponential model to the average curve\n",
    "bounds_avg = (0, [300, 300, 300, 300, 300, 300, 300])\n",
    "beta_triple_avg, _ = curve_fit(exp_model_triple, time_bleach, average_intensity_bleach,\n",
    "                               p0=[min(average_intensity_bleach), (max(average_intensity_bleach) - min(average_intensity_bleach)) / 3, 20,\n",
    "                                   (max(average_intensity_bleach) - min(average_intensity_bleach)) / 3, 50,\n",
    "                                   (max(average_intensity_bleach) - min(average_intensity_bleach)) / 3, 200],\n",
    "                               bounds=bounds_avg)\n",
    "\n",
    "# Calculate the fitted values for each model from the bleach phase onwards\n",
    "fitted_intensity_single_avg = exp_model_single(extended_time, *beta_single_avg)\n",
    "fitted_intensity_double_avg = exp_model_double(extended_time, *beta_double_avg)\n",
    "fitted_intensity_triple_avg = exp_model_triple(extended_time, *beta_triple_avg)\n",
    "\n",
    "# Calculate chi-squared values for the average curve fits\n",
    "chisq_single_avg = np.sum((average_intensity_bleach - exp_model_single(time_bleach, *beta_single_avg)) ** 2)\n",
    "chisq_double_avg = np.sum((average_intensity_bleach - exp_model_double(time_bleach, *beta_double_avg)) ** 2)\n",
    "chisq_triple_avg = np.sum((average_intensity_bleach - exp_model_triple(time_bleach, *beta_triple_avg)) ** 2)\n",
    "\n",
    "# Calculate AIC and AICc for the average curve fits\n",
    "n_avg = len(time_bleach)\n",
    "k_single_avg = 3\n",
    "k_double_avg = 5\n",
    "k_triple_avg = 7\n",
    "\n",
    "aic_single_avg = n_avg * np.log(chisq_single_avg / n_avg) + 2 * k_single_avg\n",
    "aic_double_avg = n_avg * np.log(chisq_double_avg / n_avg) + 2 * k_double_avg\n",
    "aic_triple_avg = n_avg * np.log(chisq_triple_avg / n_avg) + 2 * k_triple_avg\n",
    "\n",
    "aicc_single_avg = aic_single_avg + (2 * k_single_avg ** 2 + 2 * k_single_avg) / (n_avg - k_single_avg - 1)\n",
    "aicc_double_avg = aic_double_avg + (2 * k_double_avg ** 2 + 2 * k_double_avg) / (n_avg - k_double_avg - 1)\n",
    "aicc_triple_avg = aic_triple_avg + (2 * k_triple_avg ** 2 + 2 * k_triple_avg) / (n_avg - k_triple_avg - 1)\n",
    "\n",
    "# Display the results for the average curve fitting\n",
    "print('Results for Average Curve:')\n",
    "print(f'Single Exponential: Tau = {beta_single_avg[2]}, A = {beta_single_avg[1]}, Chi-squared = {chisq_single_avg}, AICc = {aicc_single_avg}')\n",
    "print(f'Double Exponential: Tau = {np.sort([beta_double_avg[2], beta_double_avg[4]])}, A = {np.sort([beta_double_avg[1], beta_double_avg[3]])}, Chi-squared = {chisq_double_avg}, AICc = {aicc_double_avg}')\n",
    "print(f'Triple Exponential: Tau = {np.sort([beta_triple_avg[2], beta_triple_avg[4], beta_triple_avg[6]])}, A = {np.sort([beta_triple_avg[1], beta_triple_avg[3], beta_triple_avg[5]])}, Chi-squared = {chisq_triple_avg}, AICc = {aicc_triple_avg}')\n",
    "\n",
    "# Plot the whole average curve and the fits from the bleach onwards\n",
    "plt.figure()\n",
    "plt.plot(extended_time, extended_average_intensity, 'o', label='Whole Average Data')\n",
    "plt.plot(time_bleach, exp_model_single(time_bleach, *beta_single_avg), '-', label='Single Exponential Fit', linewidth=2)\n",
    "plt.plot(time_bleach, exp_model_double(time_bleach, *beta_double_avg), '--', label='Double Exponential Fit', linewidth=2)\n",
    "#plt.plot(time_bleach, exp_model_triple(time_bleach, *beta_triple_avg), '-.', label='Triple Exponential Fit', linewidth=2)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Average Intensity')\n",
    "plt.legend()\n",
    "plt.title('Average Curve Fitting: Whole Data and Fits from Bleach Onwards')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa65fe4-48c4-4e7d-a7f6-3c1db29195b9",
   "metadata": {},
   "source": [
    "And now repeat this for the direct fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411abc2-45a3-4a59-aadb-f8126b77fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the non-outlier runs are identified, and 'outlier_runs' is available as a boolean array\n",
    "# Also assuming 'runs' is the array of intensity data, 'Frame' contains the time data, and non-outlier logic is in place\n",
    "\n",
    "# Step 1: Mask for non-outlier runs\n",
    "non_outlier_mask = ~outlier_runs  # Inverse mask for non-outliers\n",
    "non_outlier_runs = [runs[i] for i in range(len(runs)) if non_outlier_mask[i]]\n",
    "non_outlier_runs = np.array(non_outlier_runs)\n",
    "\n",
    "# Step 2: Get the average intensity ignoring NaN values\n",
    "average_intensity = np.nanmean(non_outlier_runs[:, 50:], axis=0)  # Average over the non-outlier runs\n",
    "time = Frame[50:]\n",
    "\n",
    "# Step 3: Extend time and intensity for prebleached phase\n",
    "extended_time = np.concatenate([Frame[:50], time])\n",
    "extended_average_intensity = np.concatenate([np.nanmean([run[:50] for run in non_outlier_runs], axis=0), average_intensity])\n",
    "\n",
    "# Step 4: Create mask to remove NaN values\n",
    "valid_mask = ~np.isnan(extended_average_intensity)\n",
    "extended_time = extended_time[valid_mask]\n",
    "extended_average_intensity = extended_average_intensity[valid_mask]\n",
    "\n",
    "# Step 5: Identify bleach phase and post-bleach phase\n",
    "bleach_index = np.where(Frame == 50)[0][0]\n",
    "time_bleach = extended_time[bleach_index:]\n",
    "average_intensity_bleach = extended_average_intensity[bleach_index:]\n",
    "\n",
    "# Step 6: Initialize arrays for storing fit results for the average curve\n",
    "tau_single_avg_direct = 0\n",
    "A_single_avg_direct = 0\n",
    "tau_double_avg_direct = np.zeros(2)\n",
    "A_double_avg_direct = np.zeros(2)\n",
    "tau_triple_avg_direct = np.zeros(3)\n",
    "A_triple_avg_direct = np.zeros(3)\n",
    "\n",
    "# Chi-squared and AICc values for the average curve fits\n",
    "chisq_single_avg_direct = 0\n",
    "chisq_double_avg_direct = 0\n",
    "chisq_triple_avg_direct = 0\n",
    "\n",
    "aicc_single_avg_direct = 0\n",
    "aicc_double_avg_direct = 0\n",
    "aicc_triple_avg_direct = 0\n",
    "\n",
    "# Step 7: Fit direct models on average intensity data\n",
    "\n",
    "# Single exponential model fitting for the average curve\n",
    "beta_single_avg_direct, _ = curve_fit(\n",
    "    exp_model_single, time_bleach, average_intensity_bleach,\n",
    "    p0=[min(average_intensity_bleach), max(average_intensity_bleach) - min(average_intensity_bleach), 100]\n",
    ")\n",
    "\n",
    "# Double exponential model fitting for the average curve\n",
    "beta_double_avg_direct, _ = curve_fit(\n",
    "    exp_model_double, time_bleach, average_intensity_bleach,\n",
    "    p0=[min(average_intensity_bleach), (max(average_intensity_bleach) - min(average_intensity_bleach)) / 2, 30,\n",
    "        (max(average_intensity_bleach) - min(average_intensity_bleach)) / 2, 150]\n",
    ")\n",
    "\n",
    "# Triple exponential model fitting with constraints for the average curve\n",
    "bounds_avg_direct = (0, [300, 300, 300, 300, 300, 300, 300])\n",
    "beta_triple_avg_direct, _ = curve_fit(\n",
    "    exp_model_triple, time_bleach, average_intensity_bleach,\n",
    "    p0=[min(average_intensity_bleach), (max(average_intensity_bleach) - min(average_intensity_bleach)) / 3, 20,\n",
    "        (max(average_intensity_bleach) - min(average_intensity_bleach)) / 3, 50,\n",
    "        (max(average_intensity_bleach) - min(average_intensity_bleach)) / 3, 200],\n",
    "    bounds=bounds_avg_direct\n",
    ")\n",
    "\n",
    "# Step 8: Extract tau and A values for the direct model fits\n",
    "tau_single_avg_direct = beta_single_avg_direct[2]\n",
    "A_single_avg_direct = beta_single_avg_direct[1]\n",
    "\n",
    "tau_double_avg_direct = np.sort([beta_double_avg_direct[2], beta_double_avg_direct[4]])\n",
    "A_double_avg_direct = np.array([beta_double_avg_direct[1], beta_double_avg_direct[3]])[np.argsort([beta_double_avg_direct[2], beta_double_avg_direct[4]])]\n",
    "\n",
    "tau_triple_avg_direct = np.sort([beta_triple_avg_direct[2], beta_triple_avg_direct[4], beta_triple_avg_direct[6]])\n",
    "A_triple_avg_direct = np.array([beta_triple_avg_direct[1], beta_triple_avg_direct[3], beta_triple_avg_direct[5]])[np.argsort([beta_triple_avg_direct[2], beta_triple_avg_direct[4], beta_triple_avg_direct[6]])]\n",
    "\n",
    "# Step 9: Calculate chi-squared values for the direct model fits\n",
    "fitted_intensity_single_avg_direct = exp_model_single(time_bleach, *beta_single_avg_direct)\n",
    "fitted_intensity_double_avg_direct = exp_model_double(time_bleach, *beta_double_avg_direct)\n",
    "fitted_intensity_triple_avg_direct = exp_model_triple(time_bleach, *beta_triple_avg_direct)\n",
    "\n",
    "chisq_single_avg_direct = np.sum((average_intensity_bleach - fitted_intensity_single_avg_direct) ** 2)\n",
    "chisq_double_avg_direct = np.sum((average_intensity_bleach - fitted_intensity_double_avg_direct) ** 2)\n",
    "chisq_triple_avg_direct = np.sum((average_intensity_bleach - fitted_intensity_triple_avg_direct) ** 2)\n",
    "\n",
    "# Step 10: Calculate AIC and AICc for the average curve fits\n",
    "n_avg = len(time_bleach)\n",
    "k_single_avg = 3\n",
    "k_double_avg = 5\n",
    "k_triple_avg = 7\n",
    "\n",
    "aic_single_avg_direct = n_avg * np.log(chisq_single_avg_direct / n_avg) + 2 * k_single_avg\n",
    "aic_double_avg_direct = n_avg * np.log(chisq_double_avg_direct / n_avg) + 2 * k_double_avg\n",
    "aic_triple_avg_direct = n_avg * np.log(chisq_triple_avg_direct / n_avg) + 2 * k_triple_avg\n",
    "\n",
    "aicc_single_avg_direct = aic_single_avg_direct + (2 * k_single_avg ** 2 + 2 * k_single_avg) / (n_avg - k_single_avg - 1)\n",
    "aicc_double_avg_direct = aic_double_avg_direct + (2 * k_double_avg ** 2 + 2 * k_double_avg) / (n_avg - k_double_avg - 1)\n",
    "aicc_triple_avg_direct = aic_triple_avg_direct + (2 * k_triple_avg ** 2 + 2 * k_triple_avg) / (n_avg - k_triple_avg - 1)\n",
    "\n",
    "# Step 11: Display the results for the direct model fitting on the average curve\n",
    "print('Results for Average Curve (Direct Model):')\n",
    "print(f'Single Exponential: Tau = {tau_single_avg_direct}, A = {A_single_avg_direct}, Chi-squared = {chisq_single_avg_direct}, AICc = {aicc_single_avg_direct}')\n",
    "print(f'Double Exponential: Tau = {tau_double_avg_direct}, A = {A_double_avg_direct}, Chi-squared = {chisq_double_avg_direct}, AICc = {aicc_double_avg_direct}')\n",
    "print(f'Triple Exponential: Tau = {tau_triple_avg_direct}, A = {A_triple_avg_direct}, Chi-squared = {chisq_triple_avg_direct}, AICc = {aicc_triple_avg_direct}')\n",
    "\n",
    "# Step 12: Plot the average curve and fits from bleach phase onwards\n",
    "plt.figure()\n",
    "plt.plot(extended_time, extended_average_intensity, 'o', label='Whole Average Data')\n",
    "plt.plot(time_bleach, fitted_intensity_single_avg_direct, '-', label='Single Exponential Fit', linewidth=2)\n",
    "plt.plot(time_bleach, fitted_intensity_double_avg_direct, '--', label='Double Exponential Fit', linewidth=2)\n",
    "plt.plot(time_bleach, fitted_intensity_triple_avg_direct, '-.', label='Triple Exponential Fit', linewidth=2)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Average Intensity')\n",
    "plt.legend()\n",
    "plt.title('Average Curve Fitting (Direct Model): Whole Data and Fits from Bleach Onwards')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6dbc0-8b06-4a2e-a9c2-164024441281",
   "metadata": {},
   "source": [
    "Now we need to plot all of the R values as a function of the number or runs. As usual, we start with the GBM approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4648b-6d58-4033-8d7c-ea05a3b122a2",
   "metadata": {},
   "source": [
    "Single exponential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e7f0c-3293-4027-8308-12def44491c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for Single Exponential Model\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(R_single, 'o-', color='blue', label='Single Exponential')\n",
    "plt.title('Particle Sizes for Single Exponential Model')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Particle Size (R)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d898d-09c3-4114-b931-abfdb13fe3c5",
   "metadata": {},
   "source": [
    "Double exponential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12078ed-c3e9-4a43-a083-e648761c264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot for Double Exponential Model\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(R_double1, 'o-', color='red', label='Double Exponential - Tau1')\n",
    "plt.plot(R_double2, 'o-', color='green', label='Double Exponential - Tau2')\n",
    "plt.title('Particle Sizes for Double Exponential Model')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Particle Size (R)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e0a22-f20b-4774-82c4-4b32c6911bdd",
   "metadata": {},
   "source": [
    "Triple exponential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb6a20f-e5f1-45b7-a506-b7456e2b2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot for Triple Exponential Model\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(R_triple1, 'o-', color='purple', label='Triple Exponential - Tau1')\n",
    "plt.plot(R_triple2, 'o-', color='orange', label='Triple Exponential - Tau2')\n",
    "plt.plot(R_triple3, 'o-', color='cyan', label='Triple Exponential - Tau3')\n",
    "plt.title('Particle Sizes for Triple Exponential Model')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Particle Size (R)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa13fad4-cc13-4adf-8ff7-5f339857a440",
   "metadata": {},
   "source": [
    "And we repeat the process for the direct fitting.\n",
    "Single exponential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ff474-6d0e-433f-bdec-08918dbaafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the x-axis (run indices) \n",
    "runs = np.arange(1, len(R_single_direct) + 1)\n",
    "\n",
    "# Plot for Single Exponential (non-outliers) - Direct Fitting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(runs, R_single_direct, 'o-', label=\"Single Exponential - Direct\", color='b')\n",
    "plt.title('Particle Sizes - Single Exponential Fit (Direct)')\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Particle Size (R) [units]')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da5149-e5b6-43f5-9666-14160aa5b889",
   "metadata": {},
   "source": [
    "Double exponential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c80ba8-3c8b-4d2a-a198-99b74a2f686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the lengths of runs and R_double1_direct, R_double2_direct\n",
    "print(f'Length of runs: {len(runs)}')\n",
    "print(f'Length of R_double1_direct: {len(R_double1_direct)}')\n",
    "print(f'Length of R_double2_direct: {len(R_double2_direct)}')\n",
    "\n",
    "# If they are not equal, make sure to filter runs based on valid data points\n",
    "min_len = min(len(runs), len(R_double1_direct), len(R_double2_direct))\n",
    "\n",
    "# Slice arrays to the minimum length to match dimensions\n",
    "runs_filtered = runs[:min_len]\n",
    "R_double1_direct_filtered = R_double1_direct[:min_len]\n",
    "R_double2_direct_filtered = R_double2_direct[:min_len]\n",
    "\n",
    "# Now plot the filtered data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(runs_filtered, R_double1_direct_filtered, 'o-', label=\"Double Exponential - Tau1 - Direct\", color='g')\n",
    "plt.plot(runs_filtered, R_double2_direct_filtered, 's-', label=\"Double Exponential - Tau2 - Direct\", color='r')\n",
    "plt.title('Particle Sizes - Double Exponential Fit (Direct)')\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Particle Size (R) [units]')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8e9ba-b813-48bf-88c3-24bdf7230164",
   "metadata": {},
   "source": [
    "Triple Exponential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00305371-592e-46cb-b662-65fbb2be8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the lengths of runs and R_triple1_direct, R_triple2_direct, R_triple3_direct\n",
    "print(f'Length of runs: {len(runs)}')\n",
    "print(f'Length of R_triple1_direct: {len(R_triple1_direct)}')\n",
    "print(f'Length of R_triple2_direct: {len(R_triple2_direct)}')\n",
    "print(f'Length of R_triple3_direct: {len(R_triple3_direct)}')\n",
    "\n",
    "# Ensure lengths match by filtering to the minimum length\n",
    "min_len = min(len(runs), len(R_triple1_direct), len(R_triple2_direct), len(R_triple3_direct))\n",
    "\n",
    "# Slice arrays to the minimum length to match dimensions\n",
    "runs_filtered = runs[:min_len]\n",
    "R_triple1_direct_filtered = R_triple1_direct[:min_len]\n",
    "R_triple2_direct_filtered = R_triple2_direct[:min_len]\n",
    "R_triple3_direct_filtered = R_triple3_direct[:min_len]\n",
    "\n",
    "# Plot the filtered data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(runs_filtered, R_triple1_direct_filtered, 'o-', label=\"Triple Exponential - Tau1 - Direct\", color='c')\n",
    "plt.plot(runs_filtered, R_triple2_direct_filtered, 's-', label=\"Triple Exponential - Tau2 - Direct\", color='m')\n",
    "plt.plot(runs_filtered, R_triple3_direct_filtered, 'd-', label=\"Triple Exponential - Tau3 - Direct\", color='y')\n",
    "plt.title('Particle Sizes - Triple Exponential Fit (Direct)')\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Particle Size (R) [units]')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31e385-4949-4407-9e71-9a24621e3596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54173de6-6e11-4f44-a7f0-babb0a53db92",
   "metadata": {},
   "source": [
    "Error bar test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6313ff-cab1-4803-af26-61d6e50eacdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard deviations of the particle size values\n",
    "std_R_single = np.std(R_single)\n",
    "std_R_double1 = np.std(R_double1)\n",
    "std_R_double2 = np.std(R_double2)\n",
    "std_R_triple1 = np.std(R_triple1)\n",
    "std_R_triple2 = np.std(R_triple2)\n",
    "std_R_triple3 = np.std(R_triple3)\n",
    "\n",
    "# Convert standard deviations to nanometers\n",
    "std_R_single_nm = std_R_single \n",
    "std_R_double1_nm = std_R_double1 \n",
    "std_R_double2_nm = std_R_double2 \n",
    "std_R_triple1_nm = std_R_triple1 \n",
    "std_R_triple2_nm = std_R_triple2 \n",
    "std_R_triple3_nm = std_R_triple3 \n",
    "\n",
    "# Print results\n",
    "print(f'Standard deviation of R_single: {std_R_single_nm:.2f} nm')\n",
    "print(f'Standard deviation of R_double1: {std_R_double1_nm:.2f} nm')\n",
    "print(f'Standard deviation of R_double2: {std_R_double2_nm:.2f} nm')\n",
    "print(f'Standard deviation of R_triple1: {std_R_triple1_nm:.2f} nm')\n",
    "print(f'Standard deviation of R_triple2: {std_R_triple2_nm:.2f} nm')\n",
    "print(f'Standard deviation of R_triple3: {std_R_triple3_nm:.2f} nm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66e172-03c1-4a45-bad4-ea334ce62bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error bars (3 standard deviations)\n",
    "error_R_single = 3 * std_R_single  # Error bars in meters\n",
    "error_R_single_nm = error_R_single  # Convert to nanometers\n",
    "\n",
    "# Plot for Single Exponential Model with error bars\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(range(len(R_single)), R_single , yerr=error_R_single_nm, fmt='o', color='blue', \n",
    "             label='Single Exponential', ecolor='lightblue', capsize=5, capthick=2, elinewidth=1)\n",
    "plt.title('Particle Sizes for Single Exponential Model')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Particle Size (R) [nm]')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bc6e2-5420-4c81-b812-b1df92432e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert standard deviations to nanometers\n",
    "std_R_double1_nm = std_R_double1 \n",
    "std_R_double2_nm = std_R_double2 \n",
    "\n",
    "# Calculate error bars (3 standard deviations)\n",
    "error_R_double1 = 3 * std_R_double1  # Error bars in meters\n",
    "error_R_double2 = 3 * std_R_double2  # Error bars in meters\n",
    "\n",
    "error_R_double1_nm = error_R_double1   # Convert to nanometers\n",
    "error_R_double2_nm = error_R_double2   # Convert to nanometers\n",
    "\n",
    "# Plot for Double Exponential Model with error bars\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(range(len(R_double1)), R_double1 , yerr=error_R_double1_nm, fmt='o', color='red', \n",
    "             label='Double Exponential - Tau1', ecolor='salmon', capsize=5, capthick=2, elinewidth=1)\n",
    "plt.errorbar(range(len(R_double2)), R_double2 , yerr=error_R_double2_nm, fmt='o', color='green', \n",
    "             label='Double Exponential - Tau2', ecolor='lightgreen', capsize=5, capthick=2, elinewidth=1)\n",
    "plt.title('Particle Sizes for Double Exponential Model')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Particle Size (R) [nm]')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b69217-0c38-44ba-84c3-5fc0c0d7fd85",
   "metadata": {},
   "source": [
    "what is left is to play with the error bars. USe all three methods suggested by ChatGPT and check which one is the best. Then update the fitting parts of the notebook and we should be done with a real workhorse of a script! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4effee-40f6-46af-aa4a-af998e07fed5",
   "metadata": {},
   "source": [
    "Change of opinion. Calculate all three metric without error bars for now and then discuss with Yu Chen which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff72bbab-7395-4af0-b894-c84e0f9db5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert standard deviations to nanometers\n",
    "std_R_triple1_nm = std_R_triple1 \n",
    "std_R_triple2_nm = std_R_triple2 \n",
    "std_R_triple3_nm = std_R_triple3 \n",
    "\n",
    "# Calculate error bars (3 standard deviations)\n",
    "error_R_triple1 = 3 * std_R_triple1  # Error bars in meters\n",
    "error_R_triple2 = 3 * std_R_triple2  # Error bars in meters\n",
    "error_R_triple3 = 3 * std_R_triple3  # Error bars in meters\n",
    "\n",
    "\n",
    "error_R_triple1_nm = error_R_triple1   # Convert to nanometers\n",
    "error_R_triple2_nm = error_R_triple2   # Convert to nanometers\n",
    "error_R_triple3_nm = error_R_triple3   # Convert to nanometers\n",
    "\n",
    "# Plot for Triple Exponential Model with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(range(len(R_triple1)), R_triple1, yerr=error_R_triple1_nm, fmt='o', color='blue', \n",
    "             label='Triple Exponential - Tau1', ecolor='lightblue', capsize=5, capthick=2, elinewidth=1)\n",
    "plt.errorbar(range(len(R_triple2)), R_triple2, yerr=error_R_triple2_nm, fmt='o', color='purple', \n",
    "             label='Triple Exponential - Tau2', ecolor='plum', capsize=5, capthick=2, elinewidth=1)\n",
    "plt.errorbar(range(len(R_triple3)), R_triple3, yerr=error_R_triple3_nm, fmt='o', color='orange', \n",
    "             label='Triple Exponential - Tau3', ecolor='gold', capsize=5, capthick=2, elinewidth=1)\n",
    "\n",
    "# Plot formatting\n",
    "plt.title('Particle Sizes for Triple Exponential Model')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Particle Size (R) [nm]')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3413f-df90-4183-b3c0-f4b9952ea6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89229f92-fae9-47b5-acce-198f1cc5338d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce0e9e22-4d40-43e5-bbee-649dcdf774d1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
